{
  "hash": "959ea8e4784984e913fe3f025b4d3ed3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Text processing\nformat:\n  html:\n    embed-resources: false\n    toc: true\n    toc-depth: 3\nreference-location: margin\nfreeze: true\nconda: \n    environment: easywhisper\n---\n\n`easytranscriber` supports custom regex-based text normalization functions to preprocess ASR outputs before alignment. By reconciling the outputs of the ASR model and the emissions model, the forced alignment algorithm has an opportunity to perform better.  \n\nWav2vec2 models tend to produce all lowercase or all uppercase outputs without punctuation, whereas Whisper models output mixed case text with punctuation. Whisper furthermore outputs non-verbal tokens (often within parentheses or brackets), symbols and abbreviations. \n\nNormalizing these outputs can substantially improve the quality of forced alignment. \n\n::: {.callout-note}\nThe normalizations are reversible, meaning that the original text can be recovered after forced alignment has been performed^[Whitespace is not always recoverable, depending on the regex patterns used]. \n:::\n\n## Normalization\n\nLet's apply some basic normalization to our example from A Tale of Two Cities. To explore the effect of our normalizations, we will import the [`SpanMapNormalizer`](https://github.com/kb-labb/easyaligner/blob/fd9453903b17d9176be2fc6b6df5693cff00eaf8/src/easyaligner/text/normalization.py#L48) class from the `easyaligner` library, and remove punctuation.  \n\n::: {#31a68c41 .cell execution_count=1}\n``` {.python .cell-code}\nfrom easyaligner.text.normalization import SpanMapNormalizer\n\ntext = \"\"\"Book 1. Chapter 1, The Period. It was the best of times. It was the worst of times. \nIt was the age of wisdom. It was the age of foolishness. It was the epoch of belief. \nIt was the epoch of incredulity. It was the season of light. \nIt was the season of darkness. It was the spring of hope.\"\"\"\n\nnormalizer = SpanMapNormalizer(text)\nnormalizer.transform(r\"[^\\w\\s]\", \"\")  # Remove punctuation and special characters\nprint(normalizer.current_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBook 1 Chapter 1 The Period It was the best of times It was the worst of times \nIt was the age of wisdom It was the age of foolishness It was the epoch of belief \nIt was the epoch of incredulity It was the season of light \nIt was the season of darkness It was the spring of hope\n```\n:::\n:::\n\n\nLet's make it all lowercase as well:\n\n::: {#f5df070f .cell execution_count=2}\n``` {.python .cell-code}\nnormalizer.transform(r\"\\S+\", lambda m: m.group().lower())  #\nprint(normalizer.current_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbook 1 chapter 1 the period it was the best of times it was the worst of times \nit was the age of wisdom it was the age of foolishness it was the epoch of belief \nit was the epoch of incredulity it was the season of light \nit was the season of darkness it was the spring of hope\n```\n:::\n:::\n\n\nWe may also want to convert the numbers to their word forms. The library `num2words`^[`pip install num2words`] can help with this:\n\n::: {#9d166c70 .cell execution_count=3}\n``` {.python .cell-code}\nfrom num2words import num2words\nnormalizer.transform(r\"\\d+\", lambda m: num2words(int(m.group())))  # Convert numbers to words\nprint(normalizer.current_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbook one chapter one the period it was the best of times it was the worst of times \nit was the age of wisdom it was the age of foolishness it was the epoch of belief \nit was the epoch of incredulity it was the season of light \nit was the season of darkness it was the spring of hope\n```\n:::\n:::\n\n\nWhen you're feeling done, it's a good idea to always apply whitespace normalization as the final transformation step:\n\n::: {#68616371 .cell execution_count=4}\n``` {.python .cell-code}\nnormalizer.transform(r\"\\s+\", \" \")  # Normalize whitespace to a single space\nnormalizer.transform(r\"^\\s+|\\s+$\", \"\")  # Strip leading and trailing whitespace\nprint(normalizer.current_text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbook one chapter one the period it was the best of times it was the worst of times it was the age of wisdom it was the age of foolishness it was the epoch of belief it was the epoch of incredulity it was the season of light it was the season of darkness it was the spring of hope\n```\n:::\n:::\n\n\nOur text is now ready for forced alignment. However, if we want to recover the original text, it does not suffice to pass only the normalized text to the forced alignment algorithm. We also need to pass a mapping between the original text and the normalized text. Any user supplied text normalization function in `easytranscriber` needs to return the following two objects:  \n\n::: {#a8e15cf5 .cell execution_count=5}\n``` {.python .cell-code}\nmapping = normalizer.get_token_map()\nnormalized_tokens = [item[\"normalized_token\"] for item in mapping]\n```\n:::\n\n\nLet's inspect what's in the first five items of the mapping:\n\n::: {#c3dc0d30 .cell execution_count=6}\n``` {.python .cell-code}\nfor item in mapping[:5]:\n    print(item)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'normalized_token': 'book', 'text': 'Book', 'start_char': 0, 'end_char': 4}\n{'normalized_token': 'one', 'text': '1', 'start_char': 5, 'end_char': 6}\n{'normalized_token': 'chapter', 'text': 'Chapter', 'start_char': 8, 'end_char': 15}\n{'normalized_token': 'one', 'text': '1', 'start_char': 16, 'end_char': 17}\n{'normalized_token': 'the', 'text': 'The', 'start_char': 19, 'end_char': 22}\n```\n:::\n:::\n\n\nWe can see that `normalized_token` and `text` contain the token in the normalized and original text, respectively, while `start_char` and `end_char` indicate the character indices of the token in the original text. \n\n::: {.callout-tip}\nWhen you are done testing your transformations, combine them into a function that takes in a string and outputs the normalized tokens and the mapping. See below for an example of how the default normalization function in `easytranscriber` is implemented.\n:::\n\n### Default text_normalizer\n\n`easytranscriber` provides a conservative default text normalization function. This default function is applied^[See [pipeline](../reference/pipeline.html#parameters) and the arg `text_normalizer_fn`] unless the user specificies their own function. \n\nHere is the default normalization function, for reference:\n\n::: {#4fc2a195 .cell execution_count=7}\n``` {.python .cell-code}\ndef text_normalizer(text: str) -> str:\n    \"\"\"\n    Default text normalization function.\n\n    Applies\n        - Unicode normalization (NFKC)\n        - Lowercasing\n        - Normalization of whitespace\n        - Remove parentheses and special characters\n\n    Parameters\n    ----------\n    text : str\n        Input text to normalize.\n\n    Returns\n    -------\n    tuple\n        Tuple containing (normalized_tokens, mapping).\n    \"\"\"\n    # Unicode normalization\n    normalizer = SpanMapNormalizer(text)\n    # # Remove parentheses, brackets, stars, and their content\n    # normalizer.transform(r\"\\(.*?\\)\", \"\")\n    # normalizer.transform(r\"\\[.*?\\]\", \"\")\n    # normalizer.transform(r\"\\*.*?\\*\", \"\")\n\n    # Unicode normalization on tokens and lowercasing\n    normalizer.transform(r\"\\S+\", lambda m: unicodedata.normalize(\"NFKC\", m.group()))\n    normalizer.transform(r\"\\S+\", lambda m: m.group().lower())\n    normalizer.transform(r\"[^\\w\\s]\", \"\")  # Remove punctuation and special characters\n    normalizer.transform(r\"\\s+\", \" \")  # Normalize whitespace to a single space\n    normalizer.transform(r\"^\\s+|\\s+$\", \"\")  # Strip leading and trailing whitespace\n\n    mapping = normalizer.get_token_map()\n    normalized_tokens = [item[\"normalized_token\"] for item in mapping]\n    return normalized_tokens, mapping\n```\n:::\n\n\nIn many cases you may want, or need, to be more careful with how removal of punctuation and special characters is applied. Words with hyphens, em dashes, or scores in sports games (e.g. `3-2`) are examples where you may want to insert a whitespace instead of removing the characters entirely. Beware, also, that the ordering of the transformations can sometimes matter. \n\n::: {.column-margin}\n::: {.callout-warning}\nAvoid overly broad regex patterns. A pattern that matches everything will produce a useless mapping. \n:::\n:::\n\n\nIt is highly recommended to inspect the intermediate outputs of the applied transformations as described in the [previous section](#normalization).\n\n## Sentence tokenization\n\n`easytranscriber` supports passing a tokenizer to the `pipeline` function that segments the input text according to the user's preferences. The best matching `start` and `end` timestamps will be assigned to each tokenized segment based on the outputs from forced alignment.\n\nFor sentence tokenization, we recommend using `nltk.tokenize.punkt.PunktTokenizer`. The `load_tokenizer` function from the [`easyaligner`](https://kb-labb.github.io/easyaligner/) library provides a convenient way to load an appropriate tokenizer for your language:\n\n::: {#2d512fc7 .cell execution_count=8}\n``` {.python .cell-code}\nfrom easyaligner.text import load_tokenizer\ntokenizer = load_tokenizer(language=\"swedish\")\n```\n:::\n\n\n`PunktTokenizer` maintains lists of abbreviations to avoid incorrectly splitting sentences. We can inspect the loaded tokenizer's list of abbreviations as follows:\n\n::: {#852536a6 .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"Current abbreviations:\", tokenizer._params.abbrev_types)\nprint(f\"Length of abbreviations: {len(tokenizer._params.abbrev_types)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurrent abbreviations: {'rif', 'kap', 'ital', 'dna', 'p.g.a', 'o.d', 'm.m', 'e.m', 'ppm', 'osv', 's.k', 'bf', 'jaha', 'f.m', 'hushålln.-sällsk', 'resp', 'z.b', 'föreläsn.-fören', 'ordf', 'landtm.-förb', 'o.s.v', 'åk', 'hrm', 'bl.a', 'p', 'f.d', 'ex', 'rskr', 't.o.m', 'fig', 'f.n', 'mom', 'prop', 'postst', 't.ex', 'mm', 'aig', 'm.fl', 'dir'}\nLength of abbreviations: 39\n```\n:::\n:::\n\n\nOne may however want to add custom abbreviations to the tokenizer depending on the domain of one's data:\n\n::: {#be201b18 .cell execution_count=10}\n``` {.python .cell-code}\nnew_abbreviations = {\n    \"d.v.s\",\n    \"dvs\",\n    \"fr.o.m\",\n    \"kungl\",\n    \"m.m\",\n    \"milj\",\n    \"o.s.v\",\n    \"t.o.m\",\n    \"milj.kr\",\n}\ntokenizer._params.abbrev_types.update(new_abbreviations)\nprint(\"Updated abbreviations:\", tokenizer._params.abbrev_types)\nprint(f\"Length of abbreviations: {len(tokenizer._params.abbrev_types)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUpdated abbreviations: {'rif', 'kap', 'ital', 'dna', 'p.g.a', 'o.d', 'm.m', 'e.m', 'ppm', 'osv', 's.k', 'bf', 'jaha', 'dvs', 'f.m', 'hushålln.-sällsk', 'fr.o.m', 'resp', 'z.b', 'milj', 'föreläsn.-fören', 'ordf', 'landtm.-förb', 'o.s.v', 'åk', 'hrm', 'bl.a', 'p', 'f.d', 'ex', 'rskr', 't.o.m', 'fig', 'f.n', 'mom', 'milj.kr', 'prop', 'd.v.s', 'postst', 't.ex', 'mm', 'aig', 'm.fl', 'kungl', 'dir'}\nLength of abbreviations: 45\n```\n:::\n:::\n\n\nWhen you are done: pass the tokenizer to the `pipeline` function in `easytranscriber`.  \n\n## Arbitrary tokenization\n\nThe `tokenizer` argument of the `pipeline` function accepts any function that takes in a string and outputs a list of `(start_char, end_char)` tuples. Custom tokenization functions can therefore be defined based on any arbitrary segmentation of the source text.  \n\nIn our case, however, the source text is Whisper's ASR output. Since Whisper inference is restricted to 30-second audio chunks, a coarser level of tokenization (e.g. paragraph-level) is generally infeasible. \n\nArbitrary tokenization is more relevant when aligning existing ground truth texts with longer audio segments. `easytranscriber` is built on top of [`easyaligner`](https://kb-labb.github.io/easyaligner/), which is designed for this purpose. A more in-depth guide on custom tokenization will be available in the [`easyaligner`](https://kb-labb.github.io/easyaligner/) documentation. \n\n",
    "supporting": [
      "text-processing_files"
    ],
    "filters": [],
    "includes": {}
  }
}