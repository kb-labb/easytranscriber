{
  "hash": "9ea8f64dbd7a3ac679af8c9e75f72554",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Overview\nformat:\n  html:\n    embed-resources: false\n    toc: true\n    toc-depth: 3\nlightbox: true\nreference-location: margin\nfreeze: true\nconda: \n    environment: easywhisper\n---\n\n`easytranscriber` is an automatic speech recognition library (ASR) that offers similar functionality to [`WhisperX`](https://github.com/m-bain/whisperX). Although transcription is highly optimized in most ASR libraries, there still remains ample room for improvement in several other parts of the inference pipeline. The goal of `easytranscriber` is to squeeze out as much performance as possible from these ancillary parts. This is achieved by implementing:\n\n* GPU-accelerated forced alignment using Pytorch's forced alignment API ^[There's a tutorial in Pytorch's official [documentation](https://docs.pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html). See also [Pratap et al., 2024](https://jmlr.org/papers/volume25/23-1318/23-1318.pdf#page=8)].\n* Parallel loading and pre-fetching of audio files, enabling efficient, non-blocking data loading and batching.\n* Batched inference support for wav2vec2 models (emission extraction). \n* Support for independent, parallel processing of emissions (they are written to disk).  \n\nAdditionally, `easytranscriber` supports flexible regex-based normalization of transcribed text as a means of improving forced alignment quality. The normalizations are reversible, meaning that the original text can be recovered after forced alignment. `easytranscriber` also supports using Hugging Face [`transformers`](https://github.com/huggingface/transformers) as the backend for inference. \n\n::: {.column-page-inset}\n![The easytranscriber pipeline](images/pipelines_light.svg){#fig-pipeline width=100% fig-align=\"center\"}\n:::\n\nTogether, these optimizations result in speedups of 35% to 102% compared to `WhisperX`^[See the [benchmarks](../benchmarks.qmd) page of the documentation.], depending on the hardware configuration used. \n\n## Installation\n\n### With GPU support\n\n```bash\npip install easytranscriber --extra-index-url https://download.pytorch.org/whl/cu128\n```\n\n::: {.column-margin}\n::: {.callout-tip}\nRemove `--extra-index-url` if you want a CPU-only installation.\n:::\n:::\n\n### Using uv\n\nWhen installing with [uv](https://docs.astral.sh/uv/), it will select the appropriate PyTorch version automatically (CPU for macOS, CUDA for Linux/Windows/ARM):\n\n```bash\nuv pip install easytranscriber\n```\n\n## Usage\n\nFor our quickstart guide, we will be transcribing a short clip of the first book and chapter of \"A Tale of Two Cities\" from LibriVox ^[The original recording can be found [here](https://librivox.org/a-tale-of-two-cities-by-charles-dickens-2/)].\n\n```python\nfrom pathlib import Path\n\nfrom easyaligner.text import load_tokenizer\nfrom huggingface_hub import snapshot_download\n\nfrom easytranscriber.pipelines import pipeline\nfrom easytranscriber.text.normalization import text_normalizer\n\nsnapshot_download(\n    \"Lauler/easytranscriber_tutorials\",\n    repo_type=\"dataset\",\n    local_dir=\"data/tutorials\",\n    allow_patterns=\"tale-of-two-cities_short-en/*\",  # Wildcard pattern\n    # max_workers=4,\n)\n\n\ntokenizer = load_tokenizer(\"english\") # For sentence tokenization in forced alignment\naudio_files = [file.name for file in Path(\"data/tutorials/tale-of-two-cities_short-en\").glob(\"*\")]\npipeline(\n    vad_model=\"pyannote\",\n    emissions_model=\"facebook/wav2vec2-base-960h\",\n    transcription_model=\"distil-whisper/distil-large-v3.5\",\n    audio_paths=audio_files,\n    audio_dir=\"data/tutorials/tale-of-two-cities_short-en\",\n    language=\"en\",\n    tokenizer=tokenizer,\n    text_normalizer_fn=text_normalizer,\n    cache_dir=\"models\",\n)\n```\n\n### Output\n\nBy default, `easytranscriber` outputs a JSON file for each stage of the pipeline (VAD, emissions, transcription, forced alignment). The final aligned output can be found in `output/alignments`. The directory structure will look as follows:\n\n```\noutput\n├── alignments\n├── emissions\n├── transcriptions\n└── vad\n```\n\nLet's read the final aligned output and print out one of the aligned segments. We can either read using Python's build-in `json` library, or use a convenience function provided in `easyaligner` that reads in the file as an [`AudioMetadata`](../reference/AudioMetadata.html#easytranscriber.data.datamodel.AudioMetadata) object.\n\n::: {#24f4a65d .cell execution_count=1}\n``` {.python .cell-code}\nfrom easyaligner.data.utils import read_json\nfrom pprint import pprint\n\nresults = read_json(\"output/alignments/taleoftwocities_01_dickens_64kb_trimmed.json\")\n# Print the 3rd aligned segment of the first speech\npprint(results.speeches[0].alignments[2].to_dict())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'duration': 2.02164,\n 'end': 8.57463,\n 'score': 0.99115,\n 'start': 6.55299,\n 'text': 'It was the best of times. ',\n 'words': [WordSegment(text='It ', start=6.55299, end=6.59302, score=0.99927),\n           WordSegment(text='was ', start=6.67308, end=6.77316, score=0.99967),\n           WordSegment(text='the ', start=6.85323, end=6.95331, score=0.9834),\n           WordSegment(text='best ', start=7.27357, end=7.59383, score=0.9998),\n           WordSegment(text='of ', start=7.73395, end=7.77398, score=0.99927),\n           WordSegment(text='times. ', start=7.89408, end=8.57463, score=0.96552)]}\n```\n:::\n:::\n\n\n<audio controls style=\"width: 100%\">\n  <source src=\"https://huggingface.co/datasets/Lauler/easytranscriber_tutorials/resolve/main/tale-of-two-cities_short-en/taleoftwocities_01_dickens_64kb_trimmed.mp3\"\n  type=\"audio/mpeg\">\n</audio>\n\n## Schema\n\nSee the [reference](../reference/index.qmd#data-models) page of the documentation for a detailed overview of the data models used in `easytranscriber`. Below is a simplified schema of the final output after forced alignment of our example audio file. \n\n\n::: {style=\"max-height: 460px; overflow-y: auto;\"}\n```\nAudioMetadata\n├── audio_path       \"taleoftwocities_01_dickens_64kb_trimmed.mp3\"\n├── sample_rate      16000\n├── duration         428.93\n├── metadata         null\n└── speeches[]\n    └── SpeechSegment\n        ├── speech_id       0\n        ├── start           1.769\n        ├── end             423.948\n        ├── text            null\n        ├── text_spans      null\n        ├── duration        422.179\n        ├── audio_frames    null\n        ├── probs_path      \"taleoftwocities_01_dickens_64kb_trimmed/0.npy\"\n        ├── metadata        null\n        │\n        ├── chunks[]                          ← VAD segments, transcribed by ASR\n        │   ├── [0] AudioChunk\n        │   │   ├── start         1.769\n        │   │   ├── end           28.162\n        │   │   ├── text          \"Book 1. Chapter 1, The Period. It was the\n        │   │   │                  best of times. It was the worst of times...\"\n        │   │   ├── duration      26.393\n        │   │   ├── audio_frames  422280\n        │   │   └── num_logits    1319\n        │   ├── [1] AudioChunk\n        │   │   ├── start         29.039\n        │   │   ├── end           57.085\n        │   │   └── text          \"It was the winter of despair...\"\n        │   └── ... (19 chunks total)\n        │\n        └── alignments[]                      ← sentence-level, with word timestamps\n            ├── [0] AlignmentSegment\n            │   ├── start         1.769\n            │   ├── end           2.169\n            │   ├── text          \"Book 1. \"\n            │   ├── duration      0.400\n            │   ├── score         0.482\n            │   └── words[]\n            │       ├── { text: \"Book \",  start: 1.769, end: 1.909, score: 0.964 }\n            │       └── { text: \"1. \",    start: 2.149, end: 2.169, score: 0.0   }\n            ├── [1] AlignmentSegment\n            │   ├── start         3.671\n            │   ├── end           5.112\n            │   ├── text          \"Chapter 1, The Period. \"\n            │   ├── duration      1.441\n            │   ├── score         0.737\n            │   └── words[]\n            │       ├── { text: \"Chapter \", start: 3.671, end: 3.991, score: 0.982 }\n            │       ├── { text: \"1, \",      start: 4.111, end: 4.131, score: 0.0   }\n            │       ├── { text: \"The \",     start: 4.471, end: 4.551, score: 0.972 }\n            │       └── { text: \"Period. \", start: 4.651, end: 5.112, score: 0.992 }\n            ├── [2] AlignmentSegment\n            │   ├── text          \"It was the best of times. \"\n            │   └── words[]\n            │       ├── { text: \"It \",     start: 6.553, end: 6.593, score: 0.999 }\n            │       ├── { text: \"was \",    start: 6.673, end: 6.773, score: 1.000 }\n            │       ├── { text: \"the \",    start: 6.853, end: ...                 }\n            │       └── ...\n            └── ...\n```\n:::\n\n",
    "supporting": [
      "overview_files"
    ],
    "filters": [],
    "includes": {}
  }
}