{
  "hash": "eea78e3fb6e990befd5c239925b370d9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Overview\nformat:\n  html:\n    embed-resources: false\n    toc: true\n    toc-depth: 3\nlightbox: true\nreference-location: margin\nfreeze: auto\ncache: true\nconda:\n    environment: easywhisper\nresources:\n  - ../_assets/taleoftwocities_01_dickens_64kb_trimmed.json\n---\n\n`easytranscriber` is an automatic speech recognition (ASR) library that offers similar\nfunctionality to [`WhisperX`](https://github.com/m-bain/whisperX) -- transcription with\nprecise word-level timestamps. While the transcription step itself is well-optimized in\nmost ASR libraries, the surrounding pipeline components (data loading, emission extraction,\nforced alignment) are often bottlenecks. `easytranscriber` addresses these by implementing:\n\n\n* GPU-accelerated forced alignment using Pytorch's forced alignment API ^[There's a tutorial in Pytorch's official [documentation](https://docs.pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html). See also [Pratap et al., 2024](https://jmlr.org/papers/volume25/23-1318/23-1318.pdf#page=8)].\n* Parallel loading and pre-fetching of audio files, enabling efficient, non-blocking data loading and batching.\n* Batched inference support for wav2vec2 models (emission extraction). \n* Support for independent, parallel processing of emissions (they are written to disk).  \n\nAdditionally, `easytranscriber` supports flexible regex-based normalization of transcribed text as a means of improving forced alignment quality. The normalizations are reversible, meaning that the original text can be recovered after forced alignment. `easytranscriber` also supports using Hugging Face [`transformers`](https://github.com/huggingface/transformers) as the backend for inference. \n\n::: {.column-body-outset}\n![The easytranscriber pipeline](images/pipelines_light.svg){#fig-pipeline width=100% fig-align=\"center\"}\n:::\n\nTogether, these optimizations result in speedups of 35% to 102% compared to `WhisperX`^[See the [benchmarks](../benchmarks.qmd) page of the documentation.], depending on the hardware configuration used. \n\n## Installation\n\n### With GPU support\n\n```bash\npip install easytranscriber --extra-index-url https://download.pytorch.org/whl/cu128\n```\n\n::: {.column-margin}\n::: {.callout-tip}\nRemove `--extra-index-url` if you want a CPU-only installation.\n:::\n:::\n\n### Using uv\n\nWhen installing with [uv](https://docs.astral.sh/uv/), it will select the appropriate PyTorch version automatically (CPU for macOS, CUDA for Linux/Windows/ARM):\n\n```bash\nuv pip install easytranscriber\n```\n\n## Usage\n\nFor our quickstart guide, we will be transcribing a short clip of the first book and chapter of \"A Tale of Two Cities\" from LibriVox ^[The original recording can be found [here](https://librivox.org/a-tale-of-two-cities-by-charles-dickens-2/)].\n\n::: {#ecd9e300 .cell execution_count=2}\n``` {.python .cell-code}\nfrom pathlib import Path\n\nfrom easyaligner.text import load_tokenizer\nfrom huggingface_hub import snapshot_download\n\nfrom easytranscriber.pipelines import pipeline\nfrom easytranscriber.text.normalization import text_normalizer\n\nsnapshot_download(\n    \"Lauler/easytranscriber_tutorials\",\n    repo_type=\"dataset\",\n    local_dir=\"data/tutorials\",\n    allow_patterns=\"tale-of-two-cities_short-en/*\",  # Wildcard pattern\n    # max_workers=4,\n)\n\n\ntokenizer = load_tokenizer(\"english\") # For sentence tokenization in forced alignment\naudio_files = [file.name for file in Path(\"data/tutorials/tale-of-two-cities_short-en\").glob(\"*\")]\npipeline(\n    vad_model=\"pyannote\",\n    emissions_model=\"facebook/wav2vec2-base-960h\",\n    transcription_model=\"distil-whisper/distil-large-v3.5\",\n    audio_paths=audio_files,\n    audio_dir=\"data/tutorials/tale-of-two-cities_short-en\",\n    backend=\"ct2\", # easytranscriber handles conversion between ct2 and hf formats. \n    language=\"en\",\n    tokenizer=tokenizer,\n    text_normalizer_fn=text_normalizer,\n    cache_dir=\"models\",\n)\n```\n:::\n\n\nYou can specify any repo with a Whisper model on Hugging Face. `easytranscriber` will handle the download and conversion to `ct2`^[[Ctranslate2](https://github.com/OpenNMT/CTranslate2) provides C++ optimized inference for Whisper].\n\n::: {.callout-tip}\nA list of suitable emission models for different languages can be found in the [WhisperX library](https://github.com/m-bain/whisperX/blob/d8a078eed46b139f90d7f33b472119e9be9ff969/whisperx/alignment.py#L41-L77).\n\nHugging Face `transformers` is also supported as a backend for transcription with `backend=\"hf\"`. \n:::\n\n::: {.callout-note}\nThe default VAD model is from `pyannote`. Their models are gated. To use them, you need to create a Hugging Face [access token](https://huggingface.co/docs/hub/en/security-tokens) and accept [terms and conditions](https://huggingface.co/pyannote/segmentation-3.0). Then, either i) save the access token at `~/.cache/huggingface/token` or ii) install and use the Hugging Face CLI and run `hf auth login`. See the [Hugging Face Hub quick start guide](https://huggingface.co/docs/huggingface_hub/en/quick-start#login-command) for more details. \n\nAlternatively, you can switch to `silero` VAD (CPU-only, slightly slower). Silero can be used without authentication, and performs well. See the [vad_model](../reference/pipelines.pipeline.html#parameters) parameter in the docs. \n:::\n\n### Output\n\nBy default, `easytranscriber` outputs a JSON file for each stage of the pipeline (VAD, emissions, transcription, forced alignment). The final aligned output can be found in `output/alignments`. The directory structure will look as follows:\n\n```\noutput\n├── vad                  ← SpeechSegments with AudioChunks\n├── transcriptions       ← + transcribed text per chunk\n├── emissions            ← + emission file paths (.npy)\n└── alignments           ← + AlignmentSegments with word timestamps\n```\n\n#### Demo\n\nLet's preview the results as an interactive demo. The text transcript below the audio player will automatically be highlighted in sync with the words spoken in the audio. \n\n<div id=\"audio-player\" class=\"audio-card\">\n  <div class=\"audio-card-label\">Sample audio</div>\n  <div class=\"audio-card-title\"><em>A Tale of Two Cities</em> — Chapter 1 (LibriVox)</div>\n  <audio controls>\n    <source src=\"https://huggingface.co/datasets/Lauler/easytranscriber_tutorials/resolve/main/tale-of-two-cities_short-en/taleoftwocities_01_dickens_64kb_trimmed.mp3\"\n    type=\"audio/mpeg\">\n  </audio>\n</div>\n\n<div id=\"transcript-container\" class=\"transcript-container transcript-card\"></div>\n\n::: {.column-margin}\n::: {.callout-tip}\nYou can click anywhere in the text to jump to that point (sentence) in the audio. The text is also highlighted when you drag the audio slider!\n:::\n:::\n\n<script src=\"demo.js\"></script>\n\n::: {.callout-tip}\nTo browse and search your own transcriptions with the same synchronized playback, see [easysearch](search.qmd).\n:::\n\n#### Reading the output\n\nLet's read the final aligned output and print out one of the aligned segments. We can either read it using Python's build-in `json` library, or use a convenience function provided in `easyaligner` that reads in the file as an [`AudioMetadata`](../reference/AudioMetadata.html#easytranscriber.data.datamodel.AudioMetadata) object.\n\n::: {#4360a450 .cell execution_count=3}\n``` {.python .cell-code}\nfrom easyaligner.data.utils import read_json\nfrom pprint import pprint\n\nresults = read_json(\"output/alignments/taleoftwocities_01_dickens_64kb_trimmed.json\")\n# Print the 3rd aligned segment of the first speech\npprint(results.speeches[0].alignments[2].to_dict())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'duration': 2.02164,\n 'end': 8.57463,\n 'score': 0.99115,\n 'start': 6.55299,\n 'text': 'It was the best of times. ',\n 'words': [WordSegment(text='It ', start=6.55299, end=6.59302, score=0.99927),\n           WordSegment(text='was ', start=6.67308, end=6.77316, score=0.99967),\n           WordSegment(text='the ', start=6.85323, end=6.95331, score=0.9834),\n           WordSegment(text='best ', start=7.27357, end=7.59383, score=0.9998),\n           WordSegment(text='of ', start=7.73395, end=7.77398, score=0.99927),\n           WordSegment(text='times. ', start=7.89408, end=8.57463, score=0.96552)]}\n```\n:::\n:::\n\n\n## Schema\n\nSee the [reference](../reference/index.qmd#data-models) page of the documentation for a detailed overview of the data models used in `easytranscriber`. Below is a simplified schema of the final output after forced alignment of our example audio file. \n\n\n::: {style=\"max-height: 460px; overflow-y: auto;\"}\n```\nAudioMetadata\n├── audio_path       \"taleoftwocities_01_dickens_64kb_trimmed.mp3\"\n├── sample_rate      16000\n├── duration         428.93\n├── metadata         null\n└── speeches[]\n    └── SpeechSegment\n        ├── speech_id       0\n        ├── start           1.769\n        ├── end             423.948\n        ├── text            null\n        ├── text_spans      null\n        ├── duration        422.179\n        ├── audio_frames    null\n        ├── probs_path      \"taleoftwocities_01_dickens_64kb_trimmed/0.npy\"\n        ├── metadata        null\n        │\n        ├── chunks[]                          ← VAD segments, transcribed by ASR\n        │   ├── [0] AudioChunk\n        │   │   ├── start         1.769\n        │   │   ├── end           28.162\n        │   │   ├── text          \"Book 1. Chapter 1, The Period. It was the\n        │   │   │                  best of times. It was the worst of times...\"\n        │   │   ├── duration      26.393\n        │   │   ├── audio_frames  422280\n        │   │   └── num_logits    1319\n        │   ├── [1] AudioChunk\n        │   │   ├── start         29.039\n        │   │   ├── end           57.085\n        │   │   └── text          \"It was the winter of despair...\"\n        │   └── ... (19 chunks total)\n        │\n        └── alignments[]                      ← sentence-level, with word timestamps\n            ├── [0] AlignmentSegment\n            │   ├── start         1.769\n            │   ├── end           2.169\n            │   ├── text          \"Book 1. \"\n            │   ├── duration      0.400\n            │   ├── score         0.482\n            │   └── words[]\n            │       ├── { text: \"Book \",  start: 1.769, end: 1.909, score: 0.964 }\n            │       └── { text: \"1. \",    start: 2.149, end: 2.169, score: 0.0   }\n            ├── [1] AlignmentSegment\n            │   ├── start         3.671\n            │   ├── end           5.112\n            │   ├── text          \"Chapter 1, The Period. \"\n            │   ├── duration      1.441\n            │   ├── score         0.737\n            │   └── words[]\n            │       ├── { text: \"Chapter \", start: 3.671, end: 3.991, score: 0.982 }\n            │       ├── { text: \"1, \",      start: 4.111, end: 4.131, score: 0.0   }\n            │       ├── { text: \"The \",     start: 4.471, end: 4.551, score: 0.972 }\n            │       └── { text: \"Period. \", start: 4.651, end: 5.112, score: 0.992 }\n            ├── [2] AlignmentSegment\n            │   ├── text          \"It was the best of times. \"\n            │   └── words[]\n            │       ├── { text: \"It \",     start: 6.553, end: 6.593, score: 0.999 }\n            │       ├── { text: \"was \",    start: 6.673, end: 6.773, score: 1.000 }\n            │       ├── { text: \"the \",    start: 6.853, end: ...                 }\n            │       └── ...\n            └── ...\n```\n:::\n\n",
    "supporting": [
      "overview_files"
    ],
    "filters": [],
    "includes": {}
  }
}