---
title: Text processing
format:
  html:
    embed-resources: false
    toc: true
    toc-depth: 3
reference-location: margin
freeze: true
conda: 
    environment: easywhisper
---

`easytranscriber` supports custom regex-based text normalization functions to preprocess ASR outputs before alignment. By reconciling the outputs of the ASR model and the emissions model, the forced alignment algorithm has an opportunity to perform better.  

Wav2vec2 models tend to produce all lowercase or all uppercase outputs without punctuation, whereas Whisper models output mixed case text with punctuation. Whisper furthermore outputs non-verbal tokens (often within parentheses or brackets), symbols and abbreviations. 

Normalizing these outputs can substantially improve the quality of forced alignment. 

::: {.callout-note}
The normalizations are reversible, meaning that the original text can be recovered after forced alignment has been performed^[Whitespace is not always recoverable, depending on the regex patterns used]. 
:::

## Normalization

Let's apply some basic normalization to our example from A Tale of Two Cities. To explore the effect of our normalizations, we will import the [`SpanMapNormalizer`](https://github.com/kb-labb/easyaligner/blob/fd9453903b17d9176be2fc6b6df5693cff00eaf8/src/easyaligner/text/normalization.py#L48) class from the `easyaligner` library, and remove punctuation.  

```{python}
from easyaligner.text.normalization import SpanMapNormalizer

text = """Book 1. Chapter 1, The Period. It was the best of times. It was the worst of times. 
It was the age of wisdom. It was the age of foolishness. It was the epoch of belief. 
It was the epoch of incredulity. It was the season of light. 
It was the season of darkness. It was the spring of hope."""

normalizer = SpanMapNormalizer(text)
normalizer.transform(r"[^\w\s]", "")  # Remove punctuation and special characters
print(normalizer.current_text)
```

Let's make it all lowercase as well:

```{python}
normalizer.transform(r"\S+", lambda m: m.group().lower())  #
print(normalizer.current_text)
```

We may also want to convert the numbers to their word forms. The library `num2words`^[`pip install num2words`] can help with this:

```{python}
from num2words import num2words
normalizer.transform(r"\d+", lambda m: num2words(int(m.group())))  # Convert numbers to words
print(normalizer.current_text)
```

When you're feeling done, it's a good idea to always apply whitespace normalization as the final transformation step:

```{python}
normalizer.transform(r"\s+", " ")  # Normalize whitespace to a single space
normalizer.transform(r"^\s+|\s+$", "")  # Strip leading and trailing whitespace
print(normalizer.current_text)
```

Our text is now ready for forced alignment. However, if we want to recover the original text, it does not suffice to pass only the normalized text to the forced alignment algorithm. We also need to pass a mapping between the original text and the normalized text. Any user supplied text normalization function in `easytranscriber` needs to return the following two objects:  

```{python}
mapping = normalizer.get_token_map()
normalized_tokens = [item["normalized_token"] for item in mapping]
```

Let's inspect what's in the first five items of the mapping:

```{python}
for item in mapping[:5]:
    print(item)
```

We can see that `normalized_token` and `text` contain the token in the normalized and original text, respectively, while `start_char` and `end_char` indicate the character indices of the token in the original text. 

::: {.callout-tip}
When you are done testing your transformations, combine them into a function that takes in a string and outputs the normalized tokens and the mapping. See below for an example of how the default normalization function in `easytranscriber` is implemented.
:::

### Default text_normalizer

`easytranscriber` provides a conservative default text normalization function. This default function is applied^[See [pipeline](../reference/pipeline.html#parameters) and the arg `text_normalizer_fn`] unless the user specificies their own function. 

Here is the default normalization function, for reference:

```{python}
def text_normalizer(text: str) -> str:
    """
    Default text normalization function.

    Applies
        - Unicode normalization (NFKC)
        - Lowercasing
        - Normalization of whitespace
        - Remove parentheses and special characters

    Parameters
    ----------
    text : str
        Input text to normalize.

    Returns
    -------
    tuple
        Tuple containing (normalized_tokens, mapping).
    """
    # Unicode normalization
    normalizer = SpanMapNormalizer(text)
    # # Remove parentheses, brackets, stars, and their content
    # normalizer.transform(r"\(.*?\)", "")
    # normalizer.transform(r"\[.*?\]", "")
    # normalizer.transform(r"\*.*?\*", "")

    # Unicode normalization on tokens and lowercasing
    normalizer.transform(r"\S+", lambda m: unicodedata.normalize("NFKC", m.group()))
    normalizer.transform(r"\S+", lambda m: m.group().lower())
    normalizer.transform(r"[^\w\s]", "")  # Remove punctuation and special characters
    normalizer.transform(r"\s+", " ")  # Normalize whitespace to a single space
    normalizer.transform(r"^\s+|\s+$", "")  # Strip leading and trailing whitespace

    mapping = normalizer.get_token_map()
    normalized_tokens = [item["normalized_token"] for item in mapping]
    return normalized_tokens, mapping
```

In many cases you may want, or need, to be more careful with how removal of punctuation and special characters is applied. Words with hyphens, em dashes, or scores in sports games (e.g. `3-2`) are examples where you may want to insert a whitespace instead of removing the characters entirely. Beware, also, that the ordering of the transformations can sometimes matter. 

::: {.column-margin}
::: {.callout-warning}
Avoid overly broad regex patterns. A pattern that matches everything will produce a useless mapping. 
:::
:::


It is highly recommended to inspect the intermediate outputs of the applied transformations as described in the [previous section](#normalization).

## Sentence tokenization

`easytranscriber` supports passing a tokenizer to the `pipeline` function that segments the input text according to the user's preferences. The best matching `start` and `end` timestamps will be assigned to each tokenized segment based on the outputs from forced alignment.

For sentence tokenization, we recommend using `nltk.tokenize.punkt.PunktTokenizer`. The `load_tokenizer` function from the [`easyaligner`](https://kb-labb.github.io/easyaligner/) library provides a convenient way to load an appropriate tokenizer for your language:

```{python}
from easyaligner.text import load_tokenizer
tokenizer = load_tokenizer(language="swedish")
```

`PunktTokenizer` maintains lists of abbreviations to avoid incorrectly splitting sentences. We can inspect the loaded tokenizer's list of abbreviations as follows:

```{python}
print("Current abbreviations:", tokenizer._params.abbrev_types)
print(f"Length of abbreviations: {len(tokenizer._params.abbrev_types)}")
```

One may however want to add custom abbreviations to the tokenizer depending on the domain of one's data:

```{python}
new_abbreviations = {
    "d.v.s",
    "dvs",
    "fr.o.m",
    "kungl",
    "m.m",
    "milj",
    "o.s.v",
    "t.o.m",
    "milj.kr",
}
tokenizer._params.abbrev_types.update(new_abbreviations)
print("Updated abbreviations:", tokenizer._params.abbrev_types)
print(f"Length of abbreviations: {len(tokenizer._params.abbrev_types)}")
```

When you are done: pass the tokenizer to the `pipeline` function in `easytranscriber`.  

## Arbitrary tokenization

The `tokenizer` argument of the `pipeline` function accepts any function that takes in a string and outputs a list of `(start_char, end_char)` tuples. Custom tokenization functions can therefore be defined based on any arbitrary segmentation of the source text.  

In our case, however, the source text is Whisper's ASR output. Since Whisper inference is restricted to 30-second audio chunks, a coarser level of tokenization (e.g. paragraph-level) is generally infeasible. 

Arbitrary tokenization is more relevant when aligning existing ground truth texts with longer audio segments. `easytranscriber` is built on top of [`easyaligner`](https://kb-labb.github.io/easyaligner/), which is designed for this purpose. A more in-depth guide on custom tokenization will be available in the [`easyaligner`](https://kb-labb.github.io/easyaligner/) documentation. 